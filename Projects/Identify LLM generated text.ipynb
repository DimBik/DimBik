{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ab68550",
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2023-12-12T16:50:20.682265Z",
     "iopub.status.busy": "2023-12-12T16:50:20.681863Z",
     "iopub.status.idle": "2023-12-12T16:50:21.573337Z",
     "shell.execute_reply": "2023-12-12T16:50:21.572151Z"
    },
    "papermill": {
     "duration": 0.911567,
     "end_time": "2023-12-12T16:50:21.575787",
     "exception": false,
     "start_time": "2023-12-12T16:50:20.664220",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/kaggle/input/llm-detect-ai-generated-text/sample_submission.csv\n",
      "/kaggle/input/llm-detect-ai-generated-text/train_prompts.csv\n",
      "/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\n",
      "/kaggle/input/llm-detect-ai-generated-text/train_essays.csv\n",
      "/kaggle/input/llm-essays/test_essays.csv\n",
      "/kaggle/input/llm-essays/train_essays.csv\n",
      "/kaggle/input/llmessays/essays_a.csv\n",
      "/kaggle/input/llmessays/essays_d.csv\n",
      "/kaggle/input/llmessays/essays_b.csv\n",
      "/kaggle/input/llmessays/essays_c.csv\n",
      "/kaggle/input/train-v2/train_v2_drcat_02.csv\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d9703001",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T16:50:21.609774Z",
     "iopub.status.busy": "2023-12-12T16:50:21.609249Z",
     "iopub.status.idle": "2023-12-12T16:50:36.662195Z",
     "shell.execute_reply": "2023-12-12T16:50:36.661245Z"
    },
    "papermill": {
     "duration": 15.072615,
     "end_time": "2023-12-12T16:50:36.664609",
     "exception": false,
     "start_time": "2023-12-12T16:50:21.591994",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU device not found\n",
      "Found GPU at: \n",
      "GPU not available :(\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import subprocess\n",
    "\n",
    "from nltk.corpus import wordnet\n",
    "import tensorflow as tf\n",
    "\n",
    "device_name = tf.test.gpu_device_name()\n",
    "\n",
    "#Check for GPU\n",
    "if \"GPU\" not in device_name:\n",
    "    print(\"GPU device not found\")\n",
    "    \n",
    "print('Found GPU at: {}'.format(device_name))\n",
    "\n",
    "print(\"GPU\", \"available (YESS!!!!)\" if tf.config.list_physical_devices(\"GPU\") else \"not available :(\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc0ff2e",
   "metadata": {},
   "source": [
    "Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "84a53eb3",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T16:50:36.699375Z",
     "iopub.status.busy": "2023-12-12T16:50:36.698487Z",
     "iopub.status.idle": "2023-12-12T16:50:36.704286Z",
     "shell.execute_reply": "2023-12-12T16:50:36.703320Z"
    },
    "papermill": {
     "duration": 0.025235,
     "end_time": "2023-12-12T16:50:36.706556",
     "exception": false,
     "start_time": "2023-12-12T16:50:36.681321",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize\n",
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.tokenize import word_tokenize\n",
    "import nltk\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8b9e19ec",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T16:50:36.740357Z",
     "iopub.status.busy": "2023-12-12T16:50:36.739965Z",
     "iopub.status.idle": "2023-12-12T16:50:36.753289Z",
     "shell.execute_reply": "2023-12-12T16:50:36.752205Z"
    },
    "papermill": {
     "duration": 0.032947,
     "end_time": "2023-12-12T16:50:36.755658",
     "exception": false,
     "start_time": "2023-12-12T16:50:36.722711",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#Functions\n",
    "def get_synonyms(word):\n",
    "    \"\"\"\n",
    "    Function for sunonyms.\n",
    "\n",
    "    Args\n",
    "    word (str): The word for which we want to find a sononym.\n",
    "\n",
    "    Returns\n",
    "    list: A list containing the synonyms.\n",
    "    \"\"\"\n",
    "    synonyms = []\n",
    "    for syn in wordnet.synsets(word):\n",
    "        for lemma in syn.lemmas():\n",
    "            synonyms.append(lemma.name())\n",
    "    return synonyms\n",
    "\n",
    "def replace_with_synonym(sentence):\n",
    "    \"\"\"\n",
    "    Fucntion to replace random words of a sentence with synonyms.\n",
    "\n",
    "    Args\n",
    "    sentence (str): The sentence for which we want to replace some of its words with synonyms. \n",
    "\n",
    "    Retruns\n",
    "    str: the sentence with the new words.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(sentence)\n",
    "    for i in range(len(tokens)):\n",
    "        word = tokens[i]\n",
    "        if word.isalpha():  # Process only alphabetical words\n",
    "            synonyms = get_synonyms(word)\n",
    "            if random.random()<0.4 and len(synonyms) != 0:\n",
    "                # Choose a random synonym to replace the word\n",
    "                new_word = random.choice(synonyms)\n",
    "                tokens[i] = new_word\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def delete_random_words(sentence, deletion_probability=0.05):\n",
    "    \"\"\"\n",
    "    Function which deletes some words of a sentence.\n",
    "\n",
    "    Args\n",
    "    sentence (str): the sentence from which we want to delete words.\n",
    "    deletion_probability (float): The probability that each token will be deleted.\n",
    "\n",
    "    Returns\n",
    "    str: The reformed sentence.\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(sentence)\n",
    "    result_tokens = []\n",
    "\n",
    "    for token in tokens:\n",
    "        if random.random() > deletion_probability:\n",
    "            result_tokens.append(token)\n",
    "\n",
    "    return ' '.join(result_tokens)\n",
    "\n",
    "def swap_random_words(sentence, prob = 0.1):\n",
    "    \"\"\"\n",
    "    Function that swaps words within the sentence.\n",
    "\n",
    "    Args\n",
    "    sentence (str): the sentence in which we will swap words.\n",
    "    prob (float): the probability that each token will be swapped.\n",
    "\n",
    "    Returns\n",
    "    str: the reformed sentence\n",
    "    \"\"\"\n",
    "    tokens = word_tokenize(sentence)\n",
    "\n",
    "    # Perform random swaps\n",
    "    for i in range(len(tokens)):\n",
    "        if random.random() < prob:  # 20% chance of swapping each word\n",
    "            j = random.randint(0, len(tokens) - 1)\n",
    "            x = tokens[i] \n",
    "            tokens[i] = tokens[j]\n",
    "            tokens[j] = x\n",
    "    return ' '.join(tokens)\n",
    "\n",
    "def shuffle_sentences(text):\n",
    "    \"\"\"\n",
    "    Function to shuffle the sentences of a text.\n",
    "\n",
    "    Args\n",
    "    text (str): the text that we want to shuffle its sentences.\n",
    "\n",
    "    Returns\n",
    "    str: the shuffled text\n",
    "    \"\"\"\n",
    "    sentences = sent_tokenize(text)\n",
    "    random.shuffle(sentences)\n",
    "    return ' '.join(sentences)\n",
    "\n",
    "def data_augment_general(sentence):\n",
    "    if random.random() < 0.8:\n",
    "        sentence = delete_random_words(sentence)\n",
    "    \n",
    "    sentence = swap_random_words(sentence)\n",
    "    return sentence\n",
    "\n",
    "def POS(word:str):\n",
    "    if nltk.pos_tag([word])[0][1][0].lower() in ['v','n','r']:\n",
    "        return nltk.pos_tag([word])[0][1][0].lower()\n",
    "    elif nltk.pos_tag([word])[0][1][0].lower() == 'j':\n",
    "        return 'a'\n",
    "    else:\n",
    "        return \"n\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf92c4c",
   "metadata": {},
   "source": [
    "# Text Cleaning: Delete Special Characters from the Text\n",
    "\n",
    "In this step, we will clean the text data by removing any special characters, numbers, and punctuations. This is essential to ensure that the text data is in a standardised format, which will help in better analysis and model training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0be56d96",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T16:50:36.789412Z",
     "iopub.status.busy": "2023-12-12T16:50:36.788638Z",
     "iopub.status.idle": "2023-12-12T16:50:39.009868Z",
     "shell.execute_reply": "2023-12-12T16:50:39.008671Z"
    },
    "papermill": {
     "duration": 2.241029,
     "end_time": "2023-12-12T16:50:39.012546",
     "exception": false,
     "start_time": "2023-12-12T16:50:36.771517",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "dt = pd.read_csv(\"/kaggle/input/train-v2/train_v2_drcat_02.csv\").drop(['prompt_name',\"source\",\"RDizzl3_seven\"], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "728093ef",
   "metadata": {},
   "source": [
    "We also count the number of words in each text, since it has been observed from external analysis that the LLM models use less words than human (Closer to the essays prompt). We also divide by the maximum value to transform the attributes value to [0,1] interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2826b65c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T16:50:39.385756Z",
     "iopub.status.busy": "2023-12-12T16:50:39.384994Z",
     "iopub.status.idle": "2023-12-12T16:51:13.096954Z",
     "shell.execute_reply": "2023-12-12T16:51:13.095687Z"
    },
    "papermill": {
     "duration": 33.732082,
     "end_time": "2023-12-12T16:51:13.099788",
     "exception": false,
     "start_time": "2023-12-12T16:50:39.367706",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls = []\n",
    "te = []\n",
    "for i in range(len(dt)):\n",
    "    x = re.sub(r\"[!.?;\\n,\\\"\\']\",\"\",dt['text'][i].lower())\n",
    "    te.append(x)\n",
    "    ls.append(len(x.split()))\n",
    "dt.text = te\n",
    "ls = [i/max(ls) for i in ls]\n",
    "dt['number_of_words'] = ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9ed85c75",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T16:51:13.133606Z",
     "iopub.status.busy": "2023-12-12T16:51:13.133212Z",
     "iopub.status.idle": "2023-12-12T16:51:13.146835Z",
     "shell.execute_reply": "2023-12-12T16:51:13.145608Z"
    },
    "papermill": {
     "duration": 0.033036,
     "end_time": "2023-12-12T16:51:13.149048",
     "exception": false,
     "start_time": "2023-12-12T16:51:13.116012",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dt[\"label\"].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "621aeccc",
   "metadata": {},
   "source": [
    "# Find Unique Words in Each Class\n",
    "\n",
    "Here, we will identify and extract unique words present in each class of the text data. This can help in understanding the distinct characteristics of each class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "1558f45b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T16:51:13.833014Z",
     "iopub.status.busy": "2023-12-12T16:51:13.832162Z",
     "iopub.status.idle": "2023-12-12T16:51:14.246222Z",
     "shell.execute_reply": "2023-12-12T16:51:14.245003Z"
    },
    "papermill": {
     "duration": 0.434121,
     "end_time": "2023-12-12T16:51:14.248557",
     "exception": false,
     "start_time": "2023-12-12T16:51:13.814436",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏦🤯💭💫🌟🤪😭🍁🤷💰🎢😕🏄о🏽🧠🛣🍋😬āé👦👕а🧀🥘🇷🌳🏞🎥🥪💚🏯🤖💘🏼🐶…😅💼😌👫💅🚌🌃😂🐧🤢👩💖😊⏰🍔💥🐠🏻🍗❄🕺📰🏟🥤🌨🤞🍿💨🎮🚗🤒🔍🌞📞🍽🤕🙅🥗🚨🐢🛋🎅🥛🌱♂🍕🌲🍟🍞🛫🧡🌌🤗😓🦄📚🙄🐦🍰😎🕹🧘🧙🎤🕒🇸🛍–‘️🏀‍😷🔭😴р💦🤤🙋🚀🙃📊🏠🐴🦐🌄🚂🏋🚕😆👀🙏😉🌸💉👨🗣🐟🍷🐱🚫🔥🥦─🔑🐰🛠🥕😤🧖💁🌠🤘😍🙈🙌📺💡🧦🐕💪😩🔬🍝📉🥑😲📸🎯🏥📱👍🎹👧🌧🏈\n"
     ]
    }
   ],
   "source": [
    "st = dt[dt.label == 0].sample(n = 5000, random_state=1)\n",
    "chat = dt[dt.label == 1].sample(n = 5000, random_state=1)\n",
    "human_word = set(''.join(st['text'].to_list()))\n",
    "non_human_word = set(''.join(chat['text'].to_list())) - human_word\n",
    "print(''.join(non_human_word))\n",
    "del dt\n",
    "dt = pd.concat([st,chat]).sample(frac=1, random_state=2).reset_index(drop=True)\n",
    "del st\n",
    "del chat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d88f1d90",
   "metadata": {},
   "source": [
    "# Split the dataset\n",
    "\n",
    "In this part, I am spliting the data into training (80%) and validation set (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f815c187",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T16:51:14.369222Z",
     "iopub.status.busy": "2023-12-12T16:51:14.368549Z",
     "iopub.status.idle": "2023-12-12T16:51:14.381874Z",
     "shell.execute_reply": "2023-12-12T16:51:14.380554Z"
    },
    "papermill": {
     "duration": 0.034643,
     "end_time": "2023-12-12T16:51:14.384481",
     "exception": false,
     "start_time": "2023-12-12T16:51:14.349838",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                      0         1\n",
      "0     i am a student and i believe we should transit...  0.115942\n",
      "1     hey im like totally stoked to be writing this ...  0.206522\n",
      "2     as technology grows people have more options t...  0.219203\n",
      "3     in todays world it is of paramount importance ...  0.138889\n",
      "4     luke had a really great time when he was with ...   0.18901\n",
      "...                                                 ...       ...\n",
      "7995  title: the advantages of limiting car usageliv...  0.257246\n",
      "7996  first impressions are an important part of for...   0.09843\n",
      "7997  the planets are a fasinating part of science t...  0.175121\n",
      "7998  korea is a country that has gained a lot of po...   0.19686\n",
      "7999  i do not think that the author supported his i...  0.222222\n",
      "\n",
      "[8000 rows x 2 columns]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "ar = np.asarray(dt)\n",
    "del dt\n",
    "X_train, X_validation, Y_train, Y_validation = train_test_split(ar[:,[0,2]],ar[:,1], test_size = 0.2, random_state = 2)\n",
    "del ar\n",
    "X = pd.DataFrame(X_train)\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "561f35c2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T16:51:14.421483Z",
     "iopub.status.busy": "2023-12-12T16:51:14.420384Z",
     "iopub.status.idle": "2023-12-12T16:51:14.745269Z",
     "shell.execute_reply": "2023-12-12T16:51:14.744226Z"
    },
    "papermill": {
     "duration": 0.345922,
     "end_time": "2023-12-12T16:51:14.747774",
     "exception": false,
     "start_time": "2023-12-12T16:51:14.401852",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import NearMiss\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e60166d",
   "metadata": {},
   "source": [
    "# Applying Vectorizer\n",
    "\n",
    "In this step, we will convert the text data into numerical features using a TF-IDF vectorizer. This transformation is necessary because machine learning models cannot work with raw text directly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c91a4be1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T16:51:14.784097Z",
     "iopub.status.busy": "2023-12-12T16:51:14.783415Z",
     "iopub.status.idle": "2023-12-12T16:51:28.088136Z",
     "shell.execute_reply": "2023-12-12T16:51:28.086879Z"
    },
    "papermill": {
     "duration": 13.325878,
     "end_time": "2023-12-12T16:51:28.090884",
     "exception": false,
     "start_time": "2023-12-12T16:51:14.765006",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_train = Y_train.astype(\"int\")\n",
    "vec = TfidfVectorizer(stop_words = \"english\",ngram_range=(2,3),max_features=10000)\n",
    "X1 = vec.fit_transform(X[0],Y_train).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "19e66ee8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T16:51:28.390005Z",
     "iopub.status.busy": "2023-12-12T16:51:28.389565Z",
     "iopub.status.idle": "2023-12-12T16:51:28.395846Z",
     "shell.execute_reply": "2023-12-12T16:51:28.394739Z"
    },
    "papermill": {
     "duration": 0.027418,
     "end_time": "2023-12-12T16:51:28.398125",
     "exception": false,
     "start_time": "2023-12-12T16:51:28.370707",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4043\n",
      "3957\n"
     ]
    }
   ],
   "source": [
    "print(len(Y_train[Y_train == 0]))\n",
    "print(len(Y_train[Y_train == 1]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "044c5db7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T16:51:28.435658Z",
     "iopub.status.busy": "2023-12-12T16:51:28.434823Z",
     "iopub.status.idle": "2023-12-12T16:51:28.441121Z",
     "shell.execute_reply": "2023-12-12T16:51:28.439881Z"
    },
    "papermill": {
     "duration": 0.027371,
     "end_time": "2023-12-12T16:51:28.443437",
     "exception": false,
     "start_time": "2023-12-12T16:51:28.416066",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, make_scorer\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score, GridSearchCV\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "339edd2c",
   "metadata": {},
   "source": [
    "# Train and Test the Model\n",
    "\n",
    "Finally, we will train a machine learning model using the vectorized text data and evaluate its performance on a test dataset. This step involves training the model, and then assessing its accuracy and other metrics such AUC score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "93311706",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T16:51:28.568837Z",
     "iopub.status.busy": "2023-12-12T16:51:28.568434Z",
     "iopub.status.idle": "2023-12-12T17:25:25.281268Z",
     "shell.execute_reply": "2023-12-12T17:25:25.279979Z"
    },
    "papermill": {
     "duration": 2036.751128,
     "end_time": "2023-12-12T17:25:25.300952",
     "exception": false,
     "start_time": "2023-12-12T16:51:28.549824",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM\n",
      "[[4043    0]\n",
      " [   7 3950]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "clf1 = svm.SVC(probability=True)\n",
    "clf1.fit(X1,Y_train)\n",
    "pred = clf1.predict(X1)\n",
    "print(\"SVM\")\n",
    "print(confusion_matrix(Y_train,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "6b642362",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T17:25:25.338829Z",
     "iopub.status.busy": "2023-12-12T17:25:25.338414Z",
     "iopub.status.idle": "2023-12-12T17:25:26.458454Z",
     "shell.execute_reply": "2023-12-12T17:25:26.457195Z"
    },
    "papermill": {
     "duration": 1.142294,
     "end_time": "2023-12-12T17:25:26.461104",
     "exception": false,
     "start_time": "2023-12-12T17:25:25.318810",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "Y_validation = Y_validation.astype(\"int\")\n",
    "X_val_df = pd.DataFrame(X_validation)\n",
    "X_te = vec.transform(X_val_df[0]).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "98390c66",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T17:25:26.542876Z",
     "iopub.status.busy": "2023-12-12T17:25:26.542354Z",
     "iopub.status.idle": "2023-12-12T17:25:26.552093Z",
     "shell.execute_reply": "2023-12-12T17:25:26.550976Z"
    },
    "papermill": {
     "duration": 0.030689,
     "end_time": "2023-12-12T17:25:26.554202",
     "exception": false,
     "start_time": "2023-12-12T17:25:26.523513",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e940447c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T17:25:26.679397Z",
     "iopub.status.busy": "2023-12-12T17:25:26.678907Z",
     "iopub.status.idle": "2023-12-12T17:27:08.544001Z",
     "shell.execute_reply": "2023-12-12T17:27:08.542726Z"
    },
    "papermill": {
     "duration": 101.902611,
     "end_time": "2023-12-12T17:27:08.562600",
     "exception": false,
     "start_time": "2023-12-12T17:25:26.659989",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 955,    2],\n",
       "       [  30, 1013]])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion_matrix(Y_validation, clf1.predict(X_te))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1ab3f354",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T17:27:08.600956Z",
     "iopub.status.busy": "2023-12-12T17:27:08.600524Z",
     "iopub.status.idle": "2023-12-12T17:27:08.606032Z",
     "shell.execute_reply": "2023-12-12T17:27:08.604454Z"
    },
    "papermill": {
     "duration": 0.02779,
     "end_time": "2023-12-12T17:27:08.608618",
     "exception": false,
     "start_time": "2023-12-12T17:27:08.580828",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "7ce0ccc0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T17:27:08.647481Z",
     "iopub.status.busy": "2023-12-12T17:27:08.647099Z",
     "iopub.status.idle": "2023-12-12T17:28:49.813141Z",
     "shell.execute_reply": "2023-12-12T17:28:49.811907Z"
    },
    "papermill": {
     "duration": 101.204432,
     "end_time": "2023-12-12T17:28:49.831555",
     "exception": false,
     "start_time": "2023-12-12T17:27:08.627123",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9984711731992455"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "roc_auc_score(Y_validation, clf1.predict_proba(X_te)[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3cad2ef7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T17:28:49.868399Z",
     "iopub.status.busy": "2023-12-12T17:28:49.867558Z",
     "iopub.status.idle": "2023-12-12T17:28:49.881358Z",
     "shell.execute_reply": "2023-12-12T17:28:49.880459Z"
    },
    "papermill": {
     "duration": 0.034529,
     "end_time": "2023-12-12T17:28:49.883500",
     "exception": false,
     "start_time": "2023-12-12T17:28:49.848971",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"/kaggle/input/llm-detect-ai-generated-text/test_essays.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "78f63bd6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T17:28:49.920656Z",
     "iopub.status.busy": "2023-12-12T17:28:49.919557Z",
     "iopub.status.idle": "2023-12-12T17:28:49.930454Z",
     "shell.execute_reply": "2023-12-12T17:28:49.929424Z"
    },
    "papermill": {
     "duration": 0.031861,
     "end_time": "2023-12-12T17:28:49.932883",
     "exception": false,
     "start_time": "2023-12-12T17:28:49.901022",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>prompt_id</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0000aaaa</td>\n",
       "      <td>2</td>\n",
       "      <td>Aaa bbb ccc.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1111bbbb</td>\n",
       "      <td>3</td>\n",
       "      <td>Bbb ccc ddd.</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2222cccc</td>\n",
       "      <td>4</td>\n",
       "      <td>CCC ddd eee.</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id  prompt_id          text\n",
       "0  0000aaaa          2  Aaa bbb ccc.\n",
       "1  1111bbbb          3  Bbb ccc ddd.\n",
       "2  2222cccc          4  CCC ddd eee."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "546fb3c1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T17:28:49.970660Z",
     "iopub.status.busy": "2023-12-12T17:28:49.970235Z",
     "iopub.status.idle": "2023-12-12T17:28:49.980602Z",
     "shell.execute_reply": "2023-12-12T17:28:49.979413Z"
    },
    "papermill": {
     "duration": 0.031749,
     "end_time": "2023-12-12T17:28:49.982710",
     "exception": false,
     "start_time": "2023-12-12T17:28:49.950961",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "ls = []\n",
    "te = []\n",
    "for i in range(len(test)):\n",
    "    x = re.sub(r\"[?.!;\\n,\\\"\\']\",\"\",test['text'][i].lower())\n",
    "    te.append(x)\n",
    "    ls.append(len(x.split()))\n",
    "test[\"text\"] = te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3074d7d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T17:28:50.165476Z",
     "iopub.status.busy": "2023-12-12T17:28:50.164782Z",
     "iopub.status.idle": "2023-12-12T17:28:50.171300Z",
     "shell.execute_reply": "2023-12-12T17:28:50.170399Z"
    },
    "papermill": {
     "duration": 0.029001,
     "end_time": "2023-12-12T17:28:50.173517",
     "exception": false,
     "start_time": "2023-12-12T17:28:50.144516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df = vec.transform(test.text).toarray()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "784c78e7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T17:28:50.257016Z",
     "iopub.status.busy": "2023-12-12T17:28:50.256359Z",
     "iopub.status.idle": "2023-12-12T17:28:50.263023Z",
     "shell.execute_reply": "2023-12-12T17:28:50.261999Z"
    },
    "papermill": {
     "duration": 0.028343,
     "end_time": "2023-12-12T17:28:50.265209",
     "exception": false,
     "start_time": "2023-12-12T17:28:50.236866",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "65c94051",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-12-12T17:28:50.396010Z",
     "iopub.status.busy": "2023-12-12T17:28:50.395342Z",
     "iopub.status.idle": "2023-12-12T17:28:50.556792Z",
     "shell.execute_reply": "2023-12-12T17:28:50.555826Z"
    },
    "papermill": {
     "duration": 0.183852,
     "end_time": "2023-12-12T17:28:50.559431",
     "exception": false,
     "start_time": "2023-12-12T17:28:50.375579",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "pred = clf1.predict_proba(test_df)\n",
    "with open('submission.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file, delimiter=',')\n",
    "    field = [\"id\", \"generated\"]\n",
    "    writer.writerow(field)\n",
    "    for i in range(len(test.id)):\n",
    "        check = 0\n",
    "        row = []\n",
    "        for j in test.text[i].split():\n",
    "            if j in non_human_word:\n",
    "                check = 1\n",
    "        row.append(test.id[i])\n",
    "        if check == 1:\n",
    "            row.append(1)\n",
    "        else:\n",
    "            row.append(pred[:,1][i])\n",
    "        writer.writerow(row)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "databundleVersionId": 6888007,
     "sourceId": 61542,
     "sourceType": "competition"
    },
    {
     "datasetId": 4126276,
     "sourceId": 7147497,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4146498,
     "sourceId": 7175586,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 4147309,
     "sourceId": 7176613,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30587,
   "isGpuEnabled": false,
   "isInternetEnabled": false,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 2315.736239,
   "end_time": "2023-12-12T17:28:52.955033",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2023-12-12T16:50:17.218794",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
